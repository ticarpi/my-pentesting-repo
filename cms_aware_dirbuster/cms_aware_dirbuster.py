#! /usr/bin/python
import requests
import sys
from collections import defaultdict

###################################################################################
# Todo:                                                                           #
# [+] add robots scraper > add contents of robots.txt to second wordlist          #
# [+] add CMS-aware dictionaries                                                  #
###################################################################################


# Set params or print USAGE
if len(sys.argv) < 2:
	print "Usage: $ python "+sys.argv[0]+" <base_URL> (<wordlist_file>)"
	exit(1)
baseURL = sys.argv[1].strip("/")+"/"
if len(sys.argv) == 3:
	wordlist = sys.argv[2]
else:
	wordlist = 'basic.lst'
dirlist = 'basic_dirs.lst'

resultCodes = defaultdict(list)


def runScrape(wordlist, baseURL):
	with open(dirlist) as dirs:
		    for dirname in dirs:
			dirname = dirname.rstrip()
			with open(wordlist) as filelist:
			    for filename in filelist:
				filename = filename.rstrip()
				uri = dirname+filename
				r = requests.get(baseURL+uri)
				print uri+" > "+str(r.status_code)
				resultCodes[r.status_code].append(uri)

# MAIN function
if __name__ == "__main__":
	print "\nTesting for content on:\n"+baseURL+"\nUsing the "+wordlist+" wordlist file\n"
	runScrape(wordlist, baseURL)

# Output results by status_code
for code in resultCodes:
	print "\n-----------------------------"
	print "Results for status code: "+str(code)
	print "-----------------------------"
	for uri in resultCodes[code]:
		print uri

exit(1)
